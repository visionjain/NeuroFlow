#!/usr/bin/env python3
"""
Logistic Regression Training Script for NeuroFlow
Supports Binary and Multi-class Classification with comprehensive features
"""

import sys
import json
import os
import warnings
warnings.filterwarnings('ignore')

# Check and install required libraries
def check_and_install_libraries():
    required_libraries = {
        'pandas': 'pandas',
        'numpy': 'numpy', 
        'scikit-learn': 'sklearn',
        'matplotlib': 'matplotlib',
        'seaborn': 'seaborn',
        'imbalanced-learn': 'imblearn',
        'joblib': 'joblib'
    }
    
    missing_libraries = []
    
    for lib_name, import_name in required_libraries.items():
        try:
            __import__(import_name)
        except ImportError:
            missing_libraries.append(lib_name)
    
    if missing_libraries:
        print(f"Installing required libraries: {', '.join(missing_libraries)}")
        import subprocess
        for lib in missing_libraries:
            print(f"Installing {lib}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", lib, "-q"])
        print("All libraries installed successfully!\n")

# Install libraries first
check_and_install_libraries()

# Now import all required libraries
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve, precision_recall_curve, log_loss, auc
)
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
import joblib
import time

def main():
    # Get command line arguments
    if len(sys.argv) < 2:
        print("Error: Configuration JSON required")
        sys.exit(1)
    
    try:
        config = json.loads(sys.argv[1])
    except json.JSONDecodeError as e:
        print(f"Error: Invalid JSON configuration - {e}")
        sys.exit(1)
    
    print("=" * 50)
    print("LOGISTIC REGRESSION TRAINING")
    print("=" * 50)
    print(f"Python Version: {sys.version.split()[0]}")
    print("-" * 50)
    
    # Extract configuration
    dataset_path = config.get('dataset_path', '')
    train_file = config.get('train_file', '')
    test_file = config.get('test_file')
    test_split_ratio = float(config.get('test_split_ratio', 0.2))
    selected_features = config.get('selected_features', [])
    target_column = config.get('target_column', '')
    
    # Preprocessing options
    handle_missing = config.get('handle_missing', 'Drop Rows with Missing Values')
    remove_duplicates = config.get('remove_duplicates', True)
    
    # Encoding and Scaling
    encoding_method = config.get('encoding_method', 'one-hot')
    feature_scaling = config.get('feature_scaling', None)
    
    # Model configuration
    solver = config.get('solver', 'lbfgs')
    penalty = config.get('penalty', 'l2')
    c_value = float(config.get('c_value', 1.0))
    max_iter = int(config.get('max_iter', 1000))
    random_seed = int(config.get('random_seed', 42))
    l1_ratio = float(config.get('l1_ratio', 0.5)) if penalty == 'elasticnet' else None
    
    # Class imbalance handling
    enable_imbalance = config.get('enable_imbalance', False)
    imbalance_method = config.get('imbalance_method', None)
    class_weight = config.get('class_weight', None)
    
    # Advanced options
    probability_threshold = float(config.get('probability_threshold', 0.5))
    use_stratified_split = config.get('use_stratified_split', True)
    multi_class_strategy = config.get('multi_class_strategy', 'auto')
    
    # Cross-validation
    enable_cv = config.get('enable_cv', False)
    cv_folds = int(config.get('cv_folds', 5))
    
    # Graph selection
    selected_graphs = config.get('selected_graphs', [])
    
    # Data exploration
    selected_explorations = config.get('selected_explorations', [])
    
    # Load training data
    print("\nüìÇ Loading Training Data...")
    train_path = os.path.join(dataset_path, train_file)
    
    if not os.path.exists(train_path):
        print(f"‚ùå Error: Training file not found at {train_path}")
        sys.exit(1)
    
    # Read data based on file extension
    if train_file.endswith('.csv'):
        df_train = pd.read_csv(train_path)
    elif train_file.endswith('.xlsx'):
        df_train = pd.read_excel(train_path)
    else:
        print("‚ùå Error: Unsupported file format. Use .csv or .xlsx")
        sys.exit(1)
    
    print(f"‚úÖ Training data loaded: {df_train.shape[0]} rows, {df_train.shape[1]} columns")
    
    # Data Exploration
    if selected_explorations:
        print("\n" + "=" * 50)
        print("üìä DATA EXPLORATION")
        print("=" * 50)
        
        if "First 5 Rows" in selected_explorations:
            print("\nüìã First 5 Rows:")
            print(df_train.head())
        
        if "Last 5 Rows" in selected_explorations:
            print("\nüìã Last 5 Rows:")
            print(df_train.tail())
        
        if "Dataset Shape" in selected_explorations:
            print(f"\nüìê Dataset Shape: {df_train.shape}")
        
        if "Data Types" in selected_explorations:
            print("\nüìä Data Types:")
            print(df_train.dtypes)
        
        if "Summary Statistics" in selected_explorations:
            print("\nüìà Summary Statistics:")
            print(df_train.describe())
        
        if "Missing Values" in selected_explorations:
            print("\n‚ùì Missing Values:")
            missing = df_train.isnull().sum()
            print(missing[missing > 0])
        
        if "Unique Values Per Column" in selected_explorations:
            print("\nüî¢ Unique Values Per Column:")
            print(df_train.nunique())
        
        if "Duplicate Rows" in selected_explorations:
            duplicates = df_train.duplicated().sum()
            print(f"\nüîÑ Duplicate Rows: {duplicates}")
        
        if "Target Column Distribution" in selected_explorations or "Class Distribution" in selected_explorations:
            print(f"\nüéØ Target Column Distribution ({target_column}):")
            print(df_train[target_column].value_counts())
            print(f"\nClass Balance:")
            print(df_train[target_column].value_counts(normalize=True) * 100)
    
    # Handle missing values
    print("\nüßπ Preprocessing Data...")
    if handle_missing == "Drop Rows with Missing Values":
        df_train = df_train.dropna()
        print(f"‚úÖ Dropped rows with missing values")
    elif handle_missing == "Fill with Mean":
        numeric_cols = df_train.select_dtypes(include=[np.number]).columns
        df_train[numeric_cols] = df_train[numeric_cols].fillna(df_train[numeric_cols].mean())
        print(f"‚úÖ Filled missing values with mean")
    elif handle_missing == "Fill with Median":
        numeric_cols = df_train.select_dtypes(include=[np.number]).columns
        df_train[numeric_cols] = df_train[numeric_cols].fillna(df_train[numeric_cols].median())
        print(f"‚úÖ Filled missing values with median")
    elif handle_missing == "Fill with Mode":
        for col in df_train.columns:
            df_train[col].fillna(df_train[col].mode()[0], inplace=True)
        print(f"‚úÖ Filled missing values with mode")
    
    # Remove duplicates
    if remove_duplicates:
        before = len(df_train)
        df_train = df_train.drop_duplicates()
        removed = before - len(df_train)
        if removed > 0:
            print(f"‚úÖ Removed {removed} duplicate rows")
    
    # Prepare features and target
    X = df_train[selected_features]
    y = df_train[target_column]
    
    # Detect target type
    n_classes = len(np.unique(y))
    is_binary = n_classes == 2
    print(f"\nüéØ Target Type: {'Binary' if is_binary else 'Multi-class'} Classification ({n_classes} classes)")
    print(f"   Classes: {sorted(np.unique(y))}")
    
    # Create output directory
    output_dir = os.path.join(dataset_path, f"logisticregression-{train_file.split('.')[0]}")
    os.makedirs(output_dir, exist_ok=True)
    print(f"\nüìÅ Output Directory: {output_dir}")
    
    print("\n‚úÖ Data preprocessing completed!")
    print(f"   Final dataset shape: {X.shape}")
    
    # Output generated graphs JSON marker for parsing
    print("__GENERATED_GRAPHS_JSON__" + json.dumps([]))
    
    print("\n‚úÖ Training completed successfully!")

if __name__ == "__main__":
    main()
